{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe95986b-938e-4228-be82-5f5773bf3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46799ea9-fc6d-4ae4-80b1-48995e0e3fee",
   "metadata": {},
   "source": [
    "# Decision Tree Modeling\n",
    "\n",
    "In this step we'll load the cleaned data set and then perform the modeling steps.\n",
    "\n",
    "Diabetes_012 class types:\n",
    "- 0 is for no diabetes or only during pregnancy\n",
    "- 1 is for prediabetes\n",
    "- 2 is for diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12401c42-5662-408f-b969-45c276e8c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from eda step\n",
    "file_path = \"../data/cleaned_diabetes_health_indicators_dataset.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa524b04-928c-4212-9509-b6f83418cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(\"Diabetes_012\", axis=1)\n",
    "y = df[\"Diabetes_012\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538ef001-16b4-46b9-a5b7-4b08eae51879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.98      0.91     38116\n",
      "         1.0       0.00      0.00      0.00       906\n",
      "         2.0       0.55      0.13      0.21      6935\n",
      "\n",
      "    accuracy                           0.83     45957\n",
      "   macro avg       0.47      0.37      0.37     45957\n",
      "weighted avg       0.78      0.83      0.79     45957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and a max depth of 5\n",
    "clf_entropy_5 = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
    "clf_entropy_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_entropy_5 = clf_entropy_5.predict(X_test)\n",
    "print(classification_report(y_test, y_entropy_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032c4b7b-f7b6-437f-8721-f2f76f923d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.99      0.91     38116\n",
      "         1.0       0.00      0.00      0.00       906\n",
      "         2.0       0.57      0.12      0.19      6935\n",
      "\n",
      "    accuracy                           0.83     45957\n",
      "   macro avg       0.47      0.37      0.37     45957\n",
      "weighted avg       0.78      0.83      0.78     45957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/yaroslawbagriy/Dev/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and a max depth of 5\n",
    "clf_gini_5 = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)\n",
    "clf_gini_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_gini_5 = clf_gini_5.predict(X_test)\n",
    "print(classification_report(y_test, y_gini_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f28073-7f8f-4b7d-ba10-d06b0bb077ef",
   "metadata": {},
   "source": [
    "We can see that using the decision tree with criterion set to entropy and gini with a max tree depth of 5 will be unable to predict class 1 (pre-prediabetes). Let's try the following:\n",
    "1. No max depth set\n",
    "2. Max depth of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4a5a78-c0d7-4eaa-aae1-f618413f85c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.84      0.85     38116\n",
      "         1.0       0.03      0.04      0.04       906\n",
      "         2.0       0.29      0.31      0.30      6935\n",
      "\n",
      "    accuracy                           0.75     45957\n",
      "   macro avg       0.39      0.40      0.40     45957\n",
      "weighted avg       0.76      0.75      0.75     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and no max depth\n",
    "clf_entropy_5 = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "clf_entropy_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_entropy_5 = clf_entropy_5.predict(X_test)\n",
    "print(classification_report(y_test, y_entropy_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbec8db-1039-40b1-af24-aa3635b1bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.85     38116\n",
      "         1.0       0.02      0.02      0.02       906\n",
      "         2.0       0.28      0.32      0.30      6935\n",
      "\n",
      "    accuracy                           0.74     45957\n",
      "   macro avg       0.39      0.39      0.39     45957\n",
      "weighted avg       0.76      0.74      0.75     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and no max depth\n",
    "clf_gini_5 = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf_gini_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_gini_5 = clf_gini_5.predict(X_test)\n",
    "print(classification_report(y_test, y_gini_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f511d566-89a6-40e6-b5c7-32b9d03122e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.84      0.85     38116\n",
      "         1.0       0.03      0.04      0.04       906\n",
      "         2.0       0.29      0.31      0.30      6935\n",
      "\n",
      "    accuracy                           0.75     45957\n",
      "   macro avg       0.39      0.40      0.40     45957\n",
      "weighted avg       0.76      0.75      0.75     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and max depth 50\n",
    "clf_entropy_5 = DecisionTreeClassifier(criterion='entropy', max_depth=50, random_state=42)\n",
    "clf_entropy_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_entropy_5 = clf_entropy_5.predict(X_test)\n",
    "print(classification_report(y_test, y_entropy_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "045ebb94-3110-4bcc-b777-fe364a69bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.85     38116\n",
      "         1.0       0.02      0.02      0.02       906\n",
      "         2.0       0.28      0.32      0.30      6935\n",
      "\n",
      "    accuracy                           0.74     45957\n",
      "   macro avg       0.39      0.39      0.39     45957\n",
      "weighted avg       0.76      0.74      0.75     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and max depth 50\n",
    "clf_gini_5 = DecisionTreeClassifier(criterion='gini', max_depth=50, random_state=42)\n",
    "clf_gini_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_gini_5 = clf_gini_5.predict(X_test)\n",
    "print(classification_report(y_test, y_gini_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0a521-d68e-46ae-9f83-ddfcafb7f18d",
   "metadata": {},
   "source": [
    "Due to how imbalanced this data set is the precision, recall, and f1-score is very low for class 1 and class 2. We can try to improve these results by trying the following:\n",
    "1. Set class_weight=\"balanced\". This will help give more weight to the imbalanced classes.\n",
    "2. Try controling the complexity of the decision tree by providing a maximum depth.\n",
    "3. Set min_samples_leaf, which will help the Decision Tree not overfit to the majority class (class 0, no diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "200066a2-022b-49ad-b2f2-1e89cd7486ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.65      0.75     38116\n",
      "         1.0       0.03      0.17      0.05       906\n",
      "         2.0       0.27      0.52      0.36      6935\n",
      "\n",
      "    accuracy                           0.62     45957\n",
      "   macro avg       0.40      0.45      0.39     45957\n",
      "weighted avg       0.79      0.62      0.68     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and max depth 50\n",
    "clf_entropy_5 = DecisionTreeClassifier(criterion='entropy',  class_weight='balanced', min_samples_leaf=8, max_depth=50, random_state=42)\n",
    "clf_entropy_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_entropy_5 = clf_entropy_5.predict(X_test)\n",
    "print(classification_report(y_test, y_entropy_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f6ce298-6809-4989-979c-4d76bb2e11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.59      0.72     38116\n",
      "         1.0       0.03      0.31      0.06       906\n",
      "         2.0       0.30      0.54      0.38      6935\n",
      "\n",
      "    accuracy                           0.57     45957\n",
      "   macro avg       0.42      0.48      0.39     45957\n",
      "weighted avg       0.82      0.57      0.66     45957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree using entropy as the criterion and max depth 50\n",
    "clf_gini_5 = DecisionTreeClassifier(criterion='gini',  class_weight='balanced', min_samples_leaf=8, max_depth=15, random_state=42)\n",
    "clf_gini_5.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the results\n",
    "y_gini_5 = clf_gini_5.predict(X_test)\n",
    "print(classification_report(y_test, y_gini_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d64ac2-f602-4eb3-b92f-6e510b8aa6ac",
   "metadata": {},
   "source": [
    "# Decision Tree Modeling with a 2-stage classification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3391ce3c-1477-45a5-b1c6-a0deca15dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Stage Classification Report (class 0 vs class 1+2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.67      0.78     38012\n",
      "         1.0       0.33      0.76      0.46      7945\n",
      "\n",
      "    accuracy                           0.69     45957\n",
      "   macro avg       0.63      0.72      0.62     45957\n",
      "weighted avg       0.83      0.69      0.72     45957\n",
      "\n",
      "\n",
      "Second Stage Classification Report (class 1 vs class 2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.14      0.30      0.19       608\n",
      "         2.0       0.91      0.79      0.85      5437\n",
      "\n",
      "    accuracy                           0.74      6045\n",
      "   macro avg       0.52      0.55      0.52      6045\n",
      "weighted avg       0.83      0.74      0.78      6045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge Class 1 and 2 into single class\n",
    "y_binary = y.copy()\n",
    "y_binary[y_binary == 2] = 1\n",
    "\n",
    "# Split data into training and test data, no standardization needed\n",
    "X_train, X_test, y_train_binary, y_test_binary = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "# Convert y back to Series\n",
    "y_train_binary = pd.Series(y_train_binary, index=X_train.index)\n",
    "y_test_binary = pd.Series(y_test_binary, index=X_test.index)\n",
    "\n",
    "# Train first Model: 0 vs (1+2)\n",
    "first_stage_tree = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "first_stage_tree.fit(X_train, y_train_binary)\n",
    "\n",
    "# Predict first stage\n",
    "y_pred_binary = first_stage_tree.predict(X_test)\n",
    "\n",
    "print(\"\\nFirst Stage Classification Report (class 0 vs class 1+2):\")\n",
    "print(classification_report(y_test_binary, y_pred_binary))\n",
    "\n",
    "# Predict second stage for class 1 from the binary model, then reclassify into class 1 or class 2\n",
    "\n",
    "# Find which test samples were predicted as the combined diabetes group\n",
    "# np.where always returns a tuple, get the 0th index for the actual array\n",
    "indices_pred_diabetes = np.where(y_pred_binary == 1)[0]\n",
    "\n",
    "# Subset X_test and original true labels (y_test) to only the samples predicted as diabetes\n",
    "X_test_diabetes = X_test.iloc[indices_pred_diabetes]\n",
    "# Backwards translate from binary to the original set\n",
    "y_test_full = y.iloc[y_test_binary.index]  # Go back to original labels 0/1/2\n",
    "y_test_diabetes = y_test_full.iloc[indices_pred_diabetes]\n",
    "\n",
    "# Only keep the real class 1 and 2\n",
    "X_test_diabetes = X_test_diabetes[(y_test_diabetes == 1) | (y_test_diabetes == 2)]\n",
    "y_test_diabetes = y_test_diabetes[(y_test_diabetes == 1) | (y_test_diabetes == 2)]\n",
    "\n",
    "# Train a second model to distinguish 1 vs 2\n",
    "y_train_full = y.loc[X_train.index]  # Must match original y to X_train\n",
    "mask_train_diabetes = (y_train_binary == 1)\n",
    "X_train_diabetes = X_train[mask_train_diabetes]\n",
    "y_train_diabetes = y_train_full[mask_train_diabetes]\n",
    "\n",
    "second_stage_tree = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "second_stage_tree.fit(X_train_diabetes, y_train_diabetes)\n",
    "\n",
    "# Predict second stage\n",
    "y_pred_second_stage = second_stage_tree.predict(X_test_diabetes)\n",
    "\n",
    "print(\"\\nSecond Stage Classification Report (class 1 vs class 2):\")\n",
    "print(classification_report(y_test_diabetes, y_pred_second_stage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f53674-6cca-42c0-985b-07b1759e4c4f",
   "metadata": {},
   "source": [
    "# Save final prediction data to results directroy to be used in ensemble voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f68eec-8bb8-4e4f-8c78-2c0d9d4aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from binary predictions\n",
    "y_pred_final = y_pred_binary.copy()\n",
    "\n",
    "# Get original indices of test set where model predicted diabetes\n",
    "pred_diabetes_indices = np.where(y_pred_binary == 1)[0]\n",
    "\n",
    "# Get matching original test set labels\n",
    "y_test_full = y.iloc[y_test_binary.index]  # back to 0/1/2 labels\n",
    "y_test_diabetes = y_test_full.iloc[pred_diabetes_indices]\n",
    "\n",
    "# Only keep samples that are truly class 1 or 2\n",
    "valid_indices_mask = (y_test_diabetes == 1) | (y_test_diabetes == 2)\n",
    "final_indices_to_update = pred_diabetes_indices[valid_indices_mask]\n",
    "\n",
    "# Assign back second-stage predictions\n",
    "for idx_in_list, idx in enumerate(final_indices_to_update):\n",
    "    y_pred_final[idx] = y_pred_second_stage[idx_in_list]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Save predictions\n",
    "# --------------------------------------------\n",
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(\"results/y_pred_dt.npy\", y_pred_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
